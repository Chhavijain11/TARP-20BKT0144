# -*- coding: utf-8 -*-
"""heart disease prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d6dau9dBzdiFLDmAmk0rjDS-QRubaIrv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from collections import Counter

from sklearn.multioutput import MultiOutputClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC, SVC
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier, XGBRFClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
!pip install catboost
from catboost import CatBoostClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier, VotingClassifier

from imblearn.under_sampling import RandomUnderSampler
!pip install category_encoders
from category_encoders import LeaveOneOutEncoder
from category_encoders.target_encoder import TargetEncoder
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from scipy.stats import reciprocal, uniform
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, r2_score

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("/content/drive/MyDrive/heart_statlog_cleveland_hungary_final.csv")
df.head()

print(df.columns)

#check dublicates values
df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)
df.info()

cat_cols = ['sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope', 'target']

plt.figure(figsize=(11,27))
sns.set(rc={'axes.facecolor':'#eee0e5', 'figure.facecolor':'#ffdab9'}, font_scale=0.7)
clr1 = ["#66cdaa", "#087EB0", "#2e8b57", "#cd9b9b"]
i = 0
j = 1
for col in cat_cols:
  feature = df.groupby(col)[col].count()
  plt.subplot(7, 2, i+1)
  sns.barplot(x=feature.index, y=feature.values, palette=clr1)
  plt.title(col, fontsize=15)
  plt.xlabel("")
  plt.subplot(7, 2, j+1)
  plt.pie(x=feature.values, autopct="%.1f%%", pctdistance=0.8, labels=feature.index, colors=["#66cdaa", "#087EB0", "#2e8b57", "#cd9b9b"])
  i += 2
  j += 2
  plt.show()

num_cols = ['age', 'resting bp s', 'cholesterol', 'max heart rate', 'oldpeak']
plt.figure(figsize=(10,15))
sns.set(font_scale=0.7)
i=0
j=0
for col in num_cols:
 plt.subplot(5, 2, i+1)
 sns.boxplot(df[col], color="#f08080")
 plt.title(col, fontsize=12)
 plt.xlabel("target")
 plt.subplot(5, 2, j+2)
 sns.distplot(df[col], bins=30, color="#f08080")
 plt.title(col, fontsize=12)
 plt.xlabel("targete")
 i += 2
 j += 2
plt.tight_layout()
plt.show()

#Outliers We will eliminate rows that have outliers in more than one variable
outlier_list = []
for i in num_cols:
    # Check if column exists in DataFrame
    if i in df.columns:
        Q1 = df[i].quantile(0.25)
        Q3 = df[i].quantile(0.75)
        IQR = Q3-Q1
        outlier_step = IQR * 1.5
        index_list = df[(df[i] < Q1 - outlier_step) | (df[i] > Q3 + outlier_step)].index
        outlier_list.extend(index_list)
    else:
        print(f"Warning: Column '{i}' not found in DataFrame.")

outlier_list = Counter(outlier_list)
outlier_list = list(outlier_list.items())
multi_out_list = [key for key, value in outlier_list if value > 1]

print(f"Total number of rows with outliers: {len(outlier_list)}")
print(f"Number of rows with outliers in more than one variable :{len(multi_out_list)}")

df.drop(multi_out_list, axis=0, inplace=True)
df.reset_index(drop=True, inplace=True)

#Relationship of one feature of data set with other
heart_diase_corr = df.corr()["target"]
heart_diase_corr = heart_diase_corr.drop("target", axis=0).sort_values(ascending=False)

#various graphs
plt.figure(figsize=(8,4))
sns.set(font_scale=0.8)
sns.barplot(x=heart_diase_corr.index, y=heart_diase_corr, color="#4a804d")
plt.xticks(rotation=90)
plt.title("Relationship of variables with heart diase", fontsize=15)
plt.show()

#correlation matrix
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True, linewidths=0.4, fmt='.3f', cmap="Blues",
annot_kws={'size': 8, 'rotation': 45})
plt.title("Correlation Between Features", fontsize=16)
plt.show()

#feature selection
df = pd.get_dummies(data=df, columns=["chest pain type", "resting ecg", "ST slope"])

#splitting data and training
y = df["target"]
X = df.drop("target", axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y,
random_state=42)
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols]= scaler.transform(X_test[num_cols])

#checking accuracy with various other algorithm
model_list = [LinearSVC(), LogisticRegression(), GradientBoostingClassifier(),
              MLPClassifier(max_iter=2000), AdaBoostClassifier(),
              HistGradientBoostingClassifier(),
              SVC(), XGBClassifier(), CatBoostClassifier(verbose=False)]
model_name_list = []
accuracy_list = []
for model_name in model_list:
 model = model_name
 model_cv = cross_val_score(model,
 X_train,
 y_train,
 cv=10,
 scoring= "accuracy",
 n_jobs=-1)
 model_name_list.append(model_name.__class__.__name__)
 accuracy_list.append(model_cv.mean())
 print(f"{model_name.__class__.__name__} cross validation score: {model_cv.mean()}")
 print("-" * 50)

#graphical representation of accuracy of various individual algorithm
plt.figure(figsize=(10,5))
clrs = ["brown" if i == max(accuracy_list) else "orange" for i in accuracy_list]
sns.barplot(x=accuracy_list, y=model_name_list, palette=clrs)
plt.axvline(0.8626, ls="--", lw=0.5, color="k")
plt.text(0.84,9, s="0.863", fontsize=12)
plt.title("Comparison of Models cross validation scores", fontsize=15)
plt.show()

#proposed hybrid model
gb_1 = GradientBoostingClassifier(**{"max_depth": 2,
 "min_samples_split": 5,
 "min_samples_leaf": 4,
 "learning_rate": 0.0448191306552163,
 "n_estimators": 110})
catb_2 = CatBoostClassifier(**{"depth": 6,
 "l2_leaf_reg": 8,
 "random_strength": 4,
 "subsample": 0.8263,
 "verbose": False})
voting = VotingClassifier(estimators= [("GBoosting", gb_1),
 ("CatBoost", catb_2)],
 voting="hard",
 n_jobs=-1)
voting_fit = voting.fit(X_train, y_train)

#choosing out ml algorithm for of proposed model
accuracy_list = []
for clf in (gb_1, catb_2, voting_fit):
 clf.fit(X_train, y_train)
 y_pred = clf.predict(X_test)
 acc = accuracy_score(y_test, y_pred)
 accuracy_list.append(acc)
 print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

#comparing accuracy
selected_models = ["GradientBoosting", "CatBoost", "VotingClassifier"]
plt.figure(figsize=(8,3))
clrs = ["brown" if i == max(accuracy_list) else "orange" for i in accuracy_list]
sns.barplot(x=accuracy_list, y=selected_models, palette=clrs)
plt.axvline(0.9044, ls="--", lw=0.7, color="b")
plt.text(0.86,2.80, s="0.9044", fontsize=12, color="darkred")
plt.title("Accuracy scores", fontsize=15)
plt.show()

#accrucy of proposed model
y_pred_voting = voting_fit.predict(X_test)
print(classification_report(y_test, y_pred_voting))

pred_gb = gb_1.predict(X_test)
GBoosting_cm = confusion_matrix(y_test, pred_gb)
pred_catb = catb_2.predict(X_test)
CatBoost_cm = confusion_matrix(y_test, pred_catb)
Voting_cm = confusion_matrix(y_test, y_pred_voting)
plt.figure(figsize=(15, 3))
plt.subplot(1,3,1)
sns.heatmap(GBoosting_cm, annot=True, fmt="g", cmap="Greens")
plt.title("GradientBoosting", fontsize=14)
plt.subplot(1,3,2)
sns.heatmap(CatBoost_cm, annot=True, fmt="g", cmap="Oranges")
plt.title("CatBoost", fontsize=14)
plt.subplot(1,3,3)
sns.heatmap(Voting_cm, annot=True, fmt="g", cmap="Blues")
plt.title("Voiting", fontsize=14)
plt.show()